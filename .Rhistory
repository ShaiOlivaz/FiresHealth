api_links = api_links,
folder_path = folder_path
)
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
api_links = tibble(
year = 2004:2024,
file = paste0("focos_br_todos-sats_", year, ".csv"),
url = paste0(api_links_base, file, ".zip")
)
folder_path = "../Inputs/fires/todos_sats/"
# download data from server folder
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
api_links_base =
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
api_links = tibble(
year = 2004:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file, ".zip")
)
folder_path = "../Inputs/fires/todos_sats/"
# download data from server folder
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
folder_path = "../Inputs/fires/todos_sats/"
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
View(api_links)
api_links$url
api_links_base =
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
api_links = tibble(
year = 2004:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file)
)
folder_path = "../Inputs/fires/todos_sats/"
# download data from server folder
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
api_links$url
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Personal functions
source("./Functions/bulk_rds.R")
source("./Functions/multi_download_from_df.R")
api_links_base =
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
api_links = tibble(
year = 2004:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file)
)
folder_path = "../Inputs/fires/todos_sats/"
# download data from server folder
multi_download_from_df(
api_links = api_links,
folder_path = folder_path,
cuncurrency =
)
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
multi_download_from_df(
api_links = api_links,
folder_path = folder_path,
concurrency = NULL
)
multi_download_from_df(
api_links = api_links,
folder_path = folder_path,
concurrency = 5
)
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
source("./Functions/multi_download_from_df.R")
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
View(api_links)
api_links_base =
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file)
)
folder_path = "../Inputs/fires/todos_sats/"
# download data from server folder
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df.R", echo = TRUE)
# Perform parallel download using curl::multi_download
results <- curl::multi_download(
urls = missing_links,
destfiles = missing_paths,
verbose = verbose,
resume = TRUE              # Resume if partially downloaded
)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df.R", echo = TRUE)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df.R")
library(purrr)
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file),
path = output_paths <- file.path(folder_path, api_links$file)
)
View(api_links)
View(multi_download_from_df)
View(multi_download_from_df)
View(api_links)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df.R")
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
folder_path = "../Inputs/fires/todos_sats/"
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file),
path = output_paths <- file.path(folder_path, api_links$file)
)
# download data from server folder
multi_download_from_df(
api_links = api_links,
folder_path = folder_path
)
purrr:::work2()
purrr:::walk2()
View(multi_download_from_df)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
folder_path = "../Inputs/fires/todos_sats/"
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file),
path = output_paths <- file.path(folder_path, api_links$file)
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
folder_path = "../Inputs/fires/todos_sats/"
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file),
path = output_paths <- file.path(folder_path, api_links$file)
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
multi_download_from_df_iter(api_links = api_links)
multi_download_from_df_iter(api_links = api_links)
rlang::last_trace()
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year, ".zip"),
url = paste0(api_links_base, file),
path = output_paths <- file.path(folder_path, api_links$file)
)
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
source("~/GitHub/FiresHealth/Code/Functions/multi_unzip.R")
vignette("colwise")
# unzip all files
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = if_else(
year <= 2017,
paste0("tmp/", file, ".csv")
)
)
# unzip all files
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = if_else(
year <= 2017,
paste0("tmp/", file, ".csv"),
NA
)
)
View(zip_links)
# Create dataframe with zip path links
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = if_else(
year <= 2017,
paste0("tmp/", file, ".csv"),
NA
)
)
# unzip all files
multi_unzip(zip_links = zip_links)
source("~/GitHub/FiresHealth/Code/Functions/multi_unzip.R")
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
source("~/GitHub/FiresHealth/Code/Functions/multi_unzip.R")
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
source("~/GitHub/FiresHealth/Code/Functions/multi_unzip.R")
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
source("~/GitHub/FiresHealth/Code/Functions/multi_unzip.R")
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
source("~/GitHub/FiresHealth/Code/Functions/multi_unzip.R")
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
folder_path = "../Inputs/fires/todos_sats/"
api_links = tibble(
year = 2003:2024,
file = paste0("focos_br_todos-sats_", year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
# Create dataframe with zip path links
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = if_else(
year <= 2017,
paste0("tmp/", file, ".csv"),
NA
)
)
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
View(zip_links)
source("~/GitHub/FiresHealth/Code/Functions/cleanup_zips.R")
zip_links$file
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_todos_sats/"
folder_path = "../Inputs/fires/todos_sats/"
file_base = "focos_br_todos-sats_"
api_links = tibble(
year = 2003:2024,
file = paste0(file_base, year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
# Create dataframe with zip path links
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = if_else(
year <= 2017,
paste0("tmp/", file, ".csv"),
NA
)
)
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
# remove zip files if csv is present
cleanup_zips(
file_names = zip_links$file,
folder_path = folder_path
)
source("./Functions/multi_download_from_df_iter.R")
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_sat_ref/"
folder_path = "../Inputs/fires/sat_ref/"
file_base = "focos_br_ref_"
api_links = tibble(
year = 2003:2024,
file = paste0(file_base, year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
View(api_links)
# Create dataframe with zip path links
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = if_else(
year <= 2017,
paste0("tmp/", file, ".csv"),
NA
)
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Personal functions
source("./Functions/bulk_rds.R")
source("./Functions/multi_download_from_df_iter.R")
source("./Functions/multi_unzip")
source("./Functions/multi_download_from_df_iter")
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Personal functions
source("./Functions/bulk_rds.R")
source("./Functions/multi_download_from_df_iter")
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Personal functions
source("./Functions/bulk_rds.R")
source("./Functions/multi_download_from_df_iter.R")
source("./Functions/multi_unzip.R")
source("./Functions/cleanup_zips.R")
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
# Create dataframe with zip path links
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = if_else(
year <= 2017,
paste0("tmp/", file, ".csv"),
NA
)
)
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
# unzip all files
multi_unzip(zip_links = api_links, dest_folder = folder_path)
# Create dataframe with zip path links
zip_links <- api_links |>
# .zip files up to 2017 have a tmp folder
mutate(
zip_path = NA
)
# unzip all files
multi_unzip(zip_links = zip_links, dest_folder = folder_path)
# remove zip files if csv is present
cleanup_zips(
file_names = zip_links$file,
folder_path = folder_path
)
install.packages("arrow")
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/"
folder_path = "../Inputs/sisam/"
file_base = "dados_sisam-"
api_links = tibble(
year = 2000:2019,
file = paste0(file_base, year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
rlang::last_trace()
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
View(api_links)
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/"
folder_path = "../Inputs/sisam/"
file_base = "dados_sisam-"
api_links = tibble(
year = 2000:2019,
file = paste0(file_base, year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
rlang::last_trace()
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/"
folder_path = "../Inputs/sisam/"
file_base = "dados_sisam-"
api_links = tibble(
year = 2000:2019,
file = paste0(file_base, year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
View(api_links)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
```{r}
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/"
folder_path = "../Inputs/sisam/"
file_base = "dados_sisam-"
api_links = tibble(
year = 2000:2019,
file = paste0(file_base, year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
source("~/GitHub/FiresHealth/Code/Functions/multi_download_from_df_iter.R")
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
View(api_links)
identical(
"https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/dados_sisam-2000.zip"
"https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/dados_sisam-2000.zip",
identical(
"https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/dados_sisam-2000.zip"
"https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/dados_sisam-2000.zip"
identical(
"https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/dados_sisam-2000.zip",
"https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/dados_sisam-2000.zip"
)
curl_download(
"https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/dados_sisam-2000.zip",
"../Inputs/sisam/dados_sisam-2000.zip",
quiet = FALSE
)
curl_download(
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_sat_ref/focos_br_ref_2004",
"../Inputs/sisam/dados_sisam-2000.zip",
quiet = FALSE
)
curl_download(
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_sat_ref/focos_br_ref_2004",
"../Inputs/sisam/dados_sisam-2000.zip",
quiet = FALSE
)
curl_download(
"dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_sat_ref/focos_br_ref_2004",
"../Inputs/sisam/dados_sisam-2000.zip",
quiet = FALSE
)
curl_download(
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_sat_ref/focos_br_ref_2004",
"../Inputs/",
quiet = FALSE
)
curl_download(
"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/anual/Brasil_sat_ref/focos_br_ref_2009",
"../Inputs/",
quiet = FALSE
)
knitr::opts_chunk$set(echo = TRUE)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Personal functions
source("./Functions/bulk_rds.R")
source("./Functions/multi_download_from_df_iter.R")
source("./Functions/multi_unzip.R")
source("./Functions/cleanup_zips.R")
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Personal functions
source("./Functions/bulk_rds.R")
source("./Functions/multi_download_from_df_iter.R")
source("./Functions/multi_unzip.R")
source("./Functions/cleanup_zips.R")
api_links_base = "https://dataserver-coids.inpe.br/queimadas/queimadas/sisam/"
folder_path = "../Inputs/sisam/"
file_base = "dados_sisam-"
api_links = tibble(
year = 2000:2019,
file = paste0(file_base, year),
url = paste0(api_links_base, file, ".zip"),
path = paste0(folder_path, file, ".zip")
)
# download data from server folder
multi_download_from_df_iter(api_links = api_links)
