library(duckspatial)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
ddbs_write_vector(fires_test, con = conn, table_name = "fires_test")
to_duckdb(fires_ref_sat, con = conn, table_name = "fires_ref_sat")
# sample test
fires_test <- fires_ref_sat_sample |>
select(
- c(foco_id, pais, estado, municipio)
) |>
st_as_sf(
coords = c("lon", "lat"),
crs = st_crs(muni_seat),
remove = FALSE
)
ddbs_write_vector(fires_test, con = conn, table_name = "fires_test")
## create connection
conn <- dbConnect(duckdb())
## install and load spatial extension
ddbs_install(conn)
ddbs_load(conn)
ddbs_write_vector(conn, muni_seat, "muni_seat", overwrite = T)
ddbs_write_vector(fires_test, con = conn, table_name = "fires_test", overwrite = T)
ddbs_write_vector(conn, fires_test, "fires_test", overwrite = T)
buffer = 50*1000 # in meters
ddbs_join(
"fires_test",
"muni_seat",
join = ST_DWithin(buffer),
conn = conn,
name = "fires_by_muni"
)
install.packages("pak")
pak::pak("Cidree/duckspatial")
pkgbuild::check_build_tools(debug = TRUE)
install.packages("pkgbuild")
pkgbuild::check_build_tools(debug = TRUE)
pkgbuild::check_build_tools(debug = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(stringi)
library(readr)
library(lubridate)
library(fuzzyjoin)
# Fast Data
library(arrow)
library(duckdb)
library(dbplyr)
library(duckspatial)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(stringi)
library(readr)
library(lubridate)
library(fuzzyjoin)
# Fast Data
library(arrow)
library(duckdb)
library(dbplyr)
library(duckspatial)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
# Reopen dataset from parquet file
dataset_parquet_path = "../Inputs/fires/todos_sats/parquet"
fires_all_sats <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
# Reopen dataset from parquet file
dataset_parquet_path = "../Inputs/fires/sat_ref/parquet"
fires_ref_sat <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
# Reopen dataset from parquet file
dataset_parquet_path = "../Inputs/sisam/parquet"
sisam <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
fires_ref_sat_sample <- fires_ref_sat |>
filter(year == 2016) |>
slice_sample(n = 100) |>
collect()
fires_all_sats_sample <- fires_all_sats |>
filter(year == 2016) |>
slice_sample(n = 100) |>
collect()
sisam_sample <- sisam |>
filter(year == 2016) |>
slice_sample(n = 100) |>
collect()
ddbs_write_vector(conn, muni_seat, "muni_seat", overwrite = T)
## create connection
conn <- dbConnect(duckdb())
## install and load spatial extension
ddbs_install(conn)
ddbs_load(conn)
ddbs_write_vector(conn, muni_seat, "muni_seat", overwrite = T)
bulk_read_rds(
muni_seat
)
ddbs_write_vector(conn, muni_seat, "muni_seat", overwrite = T)
# sample test
fires_test <- fires_ref_sat_sample |>
select(
- c(foco_id, pais, estado, municipio)
) |>
st_as_sf(
coords = c("lon", "lat"),
crs = st_crs(muni_seat),
remove = FALSE
)
ddbs_write_vector(conn, fires_test, "fires_test", overwrite = T)
buffer = 50*1000 # in meters
fbm <- ddbs_join(
"fires_test",
"muni_seat",
join = ST_DWithin(buffer),
conn = conn
# name = "fires_by_muni"
)
fbm <- ddbs_join(
"fires_test",
"muni_seat",
join = ST_DWithin(buffer),
conn = conn
# name = "fires_by_muni"
)
library(pak)
pak::pak("Cidree/duckspatial")
pkgbuild::check_build_tools(debug = TRUE)
install.packages("rtools")
version
pak::pak("Cidree/duckspatial")
pkgbuild::check_build_tools(debug = TRUE)
pak::pak("Cidree/duckspatial")
pkgbuild::check_build_tools(debug = TRUE)
pak::cache_clean()
knitr::opts_chunk$set(echo = TRUE)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(stringi)
library(readr)
library(lubridate)
library(fuzzyjoin)
# Fast Data
library(arrow)
library(duckdb)
library(dbplyr)
library(duckspatial)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
pak::pak("Cidree/duckspatial")
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(stringi)
library(readr)
library(lubridate)
library(fuzzyjoin)
# Fast Data
library(arrow)
library(duckdb)
library(dbplyr)
library(duckspatial)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
## create connection
conn <- dbConnect(duckdb())
## install and load spatial extension
ddbs_install(conn)
## create connection
conn <- dbConnect(duckdb())
## install and load spatial extension
# ddbs_install(conn)
ddbs_load(conn)
ddbs_install(conn)
knitr::opts_chunk$set(echo = TRUE)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(stringi)
library(readr)
library(lubridate)
library(fuzzyjoin)
# Fast Data
library(arrow)
library(duckdb)
library(dbplyr)
library(duckspatial)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
## create connection
conn <- dbConnect(duckdb())
## install and load spatial extension
ddbs_install(conn)
ddbs_load(conn)
ddbs_write_vector(conn, muni_seat, "muni_seat", overwrite = T)
bulk_read_rds(
muni_seat
)
# Reopen dataset from parquet file
dataset_parquet_path = "../Inputs/fires/todos_sats/parquet"
fires_all_sats <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
# Reopen dataset from parquet file
dataset_parquet_path = "../Inputs/fires/sat_ref/parquet"
fires_ref_sat <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
# Reopen dataset from parquet file
dataset_parquet_path = "../Inputs/sisam/parquet"
sisam <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
fires_ref_sat_sample <- fires_ref_sat |>
filter(year == 2016) |>
slice_sample(n = 100) |>
collect()
fires_all_sats_sample <- fires_all_sats |>
filter(year == 2016) |>
slice_sample(n = 100) |>
collect()
sisam_sample <- sisam |>
filter(year == 2016) |>
slice_sample(n = 100) |>
collect()
ddbs_write_vector(conn, muni_seat, "muni_seat", overwrite = T)
# sample test
fires_test <- fires_ref_sat_sample |>
select(
- c(foco_id, pais, estado, municipio)
) |>
st_as_sf(
coords = c("lon", "lat"),
crs = st_crs(muni_seat),
remove = FALSE
)
ddbs_write_vector(conn, fires_test, "fires_test", overwrite = T)
buffer = 50*1000 # in meters
fbm <- ddbs_join(
"fires_test",
"muni_seat",
join = ST_DWithin(buffer),
conn = conn
# name = "fires_by_muni"
)
fbm <- ddbs_join(
"fires_test",
"muni_seat",
join = "ST_DWithin(buffer)",
conn = conn
# name = "fires_by_muni"
)
fbm <- ddbs_join(
"fires_test",
"muni_seat",
join = "ST_DWithin",
conn = conn
# name = "fires_by_muni"
)
fbm <- ddbs_join(
"fires_test",
"muni_seat",
join = "ST_DWithin",
conn = conn
# name = "fires_by_muni"
)
fbm <- ddbs_join(
"fires_test",
"muni_seat",
join = "intersects_extent",
conn = conn
# name = "fires_by_muni"
)
View(fbm)
pairs <- st_join(
fires_test,
muni_seat,
join = st_is_within_distance,
dist = buffer, # e.g., 50km buffer
left = FALSE  # Inner join (only keep matches)
)
View(pairs)
pairs |>
select(id_bdq) |>
unique()
pairs |>
as.tbl() |>
select(id_bdq) |>
unique()
pairs |>
as_tibble() |>
select(id_bdq) |>
unique()
# Create Year and Month Columns
fires_ref_sat <- fires_ref_sat |>
mutate(
year = year(data_pas),
month = month(data_pas),
day = day(data_pas)
)
# Export as Parquet
write_dataset(
fires_ref_sat,
path = dataset_parquet_path,
format = "parquet",
partitioning = c("year")
)
View(fires_all_sats_sample)
buffer = 50*1000 # in meters
fires_ref_sat_sample <- fires_ref_sat |>
filter(year == 2016) |>
slice_sample(n = 100) |>
collect()
View(fires_ref_sat_sample)
View(fires_ref_sat)
View(fires_ref_sat_sample)
fires_test |>
st_join(
muni_seat,
join = st_is_within_distance,
dist = buffer, # e.g., 50km buffer
left = FALSE  # Inner join (only keep matches)
)
gc()
gc()
fires_by_muni_buffer50 <- fires_ref_sat |>
# Filter only relevant columns
select(
id_bdq, lat, lon, data_pas, bioma, month, year, day
) |>
# Convert to simple feature
collect() |>
st_as_sf(
coords = c("lon", "lat"),
crs = st_crs(muni_seat),
remove = FALSE
) |>
# join with municipalities, within buffer
st_join(
y = muni_seat,
join = st_is_within_distance,
dist = buffer,
left = FALSE  # Inner join (only keep matches)
) |>
st_drop_geometry()
bulk_write_rds(
fires_by_muni_buffer50
)
bulk_read_rds(
fires_by_muni_buffer50
)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(stringi)
library(readr)
library(lubridate)
library(fuzzyjoin)
# Fast Data
library(arrow)
library(duckdb)
library(dbplyr)
library(duckspatial)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
bulk_read_rds(
fires_by_muni_buffer50
)
gc()
dataset_parquet_path = "../Inputs/fires_by_muni_buffer50/parquet"
write_parquet(
fires_by_muni_buffer50,
path = dataset_parquet_path,
format = "parquet",
partitioning = c("year")
)
# Data Wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(stringi)
library(readr)
library(lubridate)
library(fuzzyjoin)
# Fast Data
library(arrow)
library(dbplyr)
# Geographic
library(sf)
library(duckspatial)
library(geoarrow)
# Personal functions
source("./Functions/bulk_rds.R")
dataset_parquet_path = "../Inputs/fires_by_muni_buffer50/parquet"
write_parquet(
fires_by_muni_buffer50,
path = dataset_parquet_path,
format = "parquet",
partitioning = c("year")
)
write_parquet(
x = fires_by_muni_buffer50,
path = dataset_parquet_path,
format = "parquet",
partitioning = c("year")
)
dataset_parquet_path = "../Inputs/fires_by_muni_buffer50/parquet"
write_parquet(
x = fires_by_muni_buffer50,
path = dataset_parquet_path,
format = "parquet",
partitioning = c("year")
)
dataset_parquet_path = "../Inputs/fires_by_muni_buffer50/parquet"
write_parquet(
x = fires_by_muni_buffer50,
path = dataset_parquet_path,
format = "parquet",
partitioning = c("year")
)
dataset_parquet_path = "../Inputs/fires_by_muni_buffer50/parquet"
# Write with group_by for RAM safety
fires_by_muni_buffer50 |>
group_by(year) |>
write_dataset(
path = dataset_parquet_path,
format = "parquet"
)
fires_by_muni_buffer50 <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
gc()
fires_by_muni_buffer50 <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
dataset_parquet_path = "../Inputs/fires_by_muni_buffer50/parquet"
fires_by_muni_buffer50 <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
sample <- fires_by_muni_buffer50 |>
filter(year == 2003) |>
slice_head(n = 100)
View(sample)
sample <- fires_by_muni_buffer50 |>
filter(year == 2003) |>
slice_head(n = 100) |>
collect()
View(sample)
fires_ref_sat <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
# Reopen dataset from parquet file
dataset_parquet_path = "../Inputs/fires/sat_ref/parquet"
fires_ref_sat <- open_dataset(
dataset_parquet_path,
format = "parquet"
)
fires_ref_sat |>
select(id_bdq) |>
summarise(n()) |>
collect()
